# HackatonIBM
## PrÃ©sentation des jeux de donnÃ©es ğŸ“Š

Pour ce data challenge, nous avons dâ€™abord disposÃ© de trois jeux de donnÃ©es au format CSV :  
- *cards_data.csv* , regroupant les informations relatives aux cartes (type, date dâ€™Ã©mission, limites, etc.)  
- *users_data.csv* , contenant les attributs des utilisateurs (Ã¢ge, genre, localisation, statut)  
- *transactions_train.csv*, listant les opÃ©rations effectuÃ©es (montant, date, moyen de paiement, marchand)  

Chacun de ces fichiers prÃ©sentait des colonnes complÃ©mentaires ; nous avons donc procÃ©dÃ© Ã  leur fusion (merge) sur les clÃ©s communes (ID carte et ID utilisateur) afin de constituer un seul jeu de donnÃ©es unifiÃ©, prÃªt pour lâ€™analyse et le prÃ©traitement. ğŸš€

## Ã‰tiquetage de la fraude ğŸš©

Les Ã©tiquettes de fraude (fraud/no-fraud) Ã©taient, quant Ã  elles, fournies dans un fichier JSON Ã  part, nommÃ© *train_fraud_labels.json* ğŸ—‚ï¸.  
Ce fichier associe Ã  chaque identifiant de transaction son label de fraude, nous permettant ainsi de superviser lâ€™apprentissage des modÃ¨les de dÃ©tection. ğŸ”
# ğŸ“Š RÃ©sumÃ© des Performances du ModÃ¨le LGBM â€“ DÃ©tection de Fraude

**ğŸ”¢ Nombre d'instances Ã©valuÃ©es** : 21â€¯000  
**ğŸ§  Algorithme utilisÃ©** : LightGBM (LGBMClassifier)  
**ğŸ§® Nombre de caractÃ©ristiques** : 46  
**ğŸ¯ Colonne cible** : `target` (1 = fraude, 0 = non-fraude)

---

## Feature Engineering ğŸ› ï¸

- Suppression des colonnes inutiles, telles que **cvv** ğŸ”’ (risque de confidentialitÃ©) et autres attributs peu informatifs  
- Application du **One-Hot Encoding** pour les variables catÃ©gorielles, transformant chaque modalitÃ© en une colonne binaire distincte ğŸ·ï¸

---

## âœ… Scores d'Ã‰valuation

| **MÃ©trique**               | **Holdout** | **Validation CroisÃ©e** |
|----------------------------|-------------|-------------------------|
| **Exactitude (Accuracy)**  | 0.999       | 0.999                   |
| **AUC-ROC**                | 0.996       | 0.985                   |
| **PrÃ©cision**              | 0.767       | 0.687                   |
| **Rappel (Recall)**        | 0.742       | 0.743                   |
| **F1-Score**               | 0.754       | 0.711                   |
| **PrÃ©cision Moyenne (AP)** | 0.786       | 0.749                   |
| **Log Loss**               | 0.003       | 0.004                   |

---

## ğŸ“Œ Matrice de Confusion

| ObservÃ©       | PrÃ©dit 1 | PrÃ©dit 0 | % Correct |
|---------------|----------|----------|-----------|
| 1 (Fraude)    | 23       | 8        | 74.2%     |
| 0 (Non-fraude)| 7        | 20â€¯962   | 100.0%    |

---

## ğŸ§  InterprÃ©tation des RÃ©sultats

- **PrÃ©cision Ã©levÃ©e** : Le modÃ¨le atteint une prÃ©cision de 76.7% pour la classe fraude, ce qui indique une bonne capacitÃ© Ã  identifier les transactions frauduleuses.
- **Rappel solide** : Avec un rappel de 74.2%, le modÃ¨le dÃ©tecte une proportion significative des fraudes rÃ©elles.
- **AUC-ROC Ã©levÃ©** : Un score AUC-ROC de 0.996 en holdout suggÃ¨re une excellente capacitÃ© de discrimination entre les classes.
- **Log Loss faible** : Une perte logarithmique de 0.003 indique que le modÃ¨le est bien calibrÃ© et confiant dans ses prÃ©dictions.

---

## ğŸ› ï¸ Recommandations

- **Analyse des erreurs** : Ã‰tudiez les cas de fausses alertes (faux positifs) et de fraudes non dÃ©tectÃ©es (faux nÃ©gatifs) pour identifier des motifs ou des caractÃ©ristiques communes.
- **Ajustement du seuil de dÃ©cision** : En fonction de la tolÃ©rance au risque, envisagez d'ajuster le seuil de classification pour Ã©quilibrer davantage la prÃ©cision et le rappel.
- **Surveillance continue** : Mettez en place une surveillance continue des performances du modÃ¨le pour dÃ©tecter toute dÃ©rive ou changement dans les donnÃ©es entrantes.

Cette premiere pipeline permet d'avoir les meilleurs compromis entre la prÃ©cision et le rappel, cependant Ã©tant donnÃ© le problÃ¨me il peut Ãªtre pertinent de se concentrer sur le rappel.
Voici les performances d'une autre pipeline qui permet d'atteindre un rappel parfait mais une prÃ©cision faible.
(peut-Ãªtre mieux en function de l'usage du modÃ¨le)

# ğŸ“Š Model Summary â€“ Decision Tree Classifier

## ğŸ§ª Experiment Details

- **Prediction column**: `target`
- **Algorithm**: Decision Tree Classifier
- **Number of features**: 76
- **Evaluation instances**: 21,000


---

## ğŸ“ˆ Evaluation Metrics

### âœ… Holdout Metrics (single train/test split)

| Metric              | Value  |
|---------------------|--------|
| Accuracy            | 0.860  |
| Area under ROC      | 0.936  |
| Precision           | 0.010  |
| Recall              | 1.000  |
| F1 Score            | 0.021  |
| Average Precision   | 0.011  |
| Log Loss            | 0.307  |

---

### ğŸ” Cross-Validation Metrics

| Metric              | Value  |
|---------------------|--------|
| Accuracy            | 0.865  |
| Area under ROC      | 0.914  |
| Precision           | 0.011  |
| Recall              | 0.954  |
| F1 Score            | 0.021  |
| Average Precision   | 0.011  |
| Log Loss            | 0.299  |

---

## ğŸ“‰ Confusion Matrix Summary (Holdout)

| Actual \ Predicted | 1 (Fraud) | 0 (Non-fraud) | % Correct |
|--------------------|-----------|---------------|-----------|
| 1 (Fraud)          | 31        | 0             | 100.0%    |
| 0 (Non-fraud)      | 2937      | 18,032        | 86.0%     |

- **Class 1 (Fraud)** Recall: **100%**
- **Very low precision (1%)**: The model detects all fraud cases, but with many false positives.

---

## ğŸ“ Interpretation

- The Decision Tree model **detects all fraud cases (100% recall)**, which is important in fraud detection.
- However, the **precision is extremely low (1%)**, meaning it misclassifies many normal transactions as fraud.
- This leads to a **very high number of false positives**, which may cause operational inefficiency or alert fatigue.

ğŸ‘‰ Consider using more advanced models (like LGBM or Random Forest) or adding cost-sensitive learning to improve the precision.